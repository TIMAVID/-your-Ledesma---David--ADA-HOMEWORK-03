---
title: "<Ledesma>-<David>-HOMEWORK-03"
author: "David Ledesma"
date: "3/20/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Problem 1
```{r}
x <- c(1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 
        1, 0)

y <- c(1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 
        0, 1, 1, 0, 1, 1, 1)
p0 <-.8
Z.prop.test <- function(x, p0, y = NULL, alternative = "two.sided", conf.level = .95) #where x is a vector, p0 is the expected porportion, y is a second vector, alternative=("two.sided" for two tailed, "greater" for upper tail, "less" for lower tail), confidence level is .95
  {
  if ((length(x) * p0 < 5) && (length(x) * ( 1 - p0 ) < 5)) stop("check if normal distribution")
  if (is.null(y)) {
    z<- (mean(x) - p0)/sqrt(p0 * (1 - p0)/length(x))
    ci <- c(mean(x) - qnorm(conf.level + 0.025) * sqrt(mean(x) * (1 - mean(x))/length(x)), 
            mean(x) + qnorm(conf.level + 0.025) * sqrt(mean(x) * (1 - mean(x))/length(x)))
  if (alternative <= "two.sided"){
    p <- (pnorm(z, lower.tail = FALSE) + (1 - pnorm(z, lower.tail = TRUE)))}
  if (alternative <= "greater"){
    p <- 1 - pnorm(z, lower.tail = FALSE)} 
  if (alternative <="less"){
    p <- pnorm(z, lower.tail = TRUE)} 
  print(list("z statistic" = z, "confidence interval" = ci, "p value" = p))
  }
  if (!is.null(y)) { 
    z<- (mean(y) - mean(x))/sqrt(((sum(x) + sum(y))/(length(x) + length(y)) * 
              (1 - (sum(x) + sum(y))/(length(x) + length(y)))) * 
             (1/length(x) + 1/length(y)))
  if (alternative <= "two.sided") {
      p <- pnorm(z, lower.tail = FALSE) + (1 - pnorm(z, lower.tail = TRUE))
      }
  if (alternative <= "greater"){
      p <- 1 - pnorm(z, lower.tail = FALSE)} 
  if (alternative <="less"){
      p <- pnorm(z, lower.tail = TRUE)} 
  m1<-mean(x)
  m2<-mean(y)
  se <- sqrt(sd(x) *sd(y)/length(x)+sd(y)*sd(y)/length(y))
  error <- qnorm(0.975)*se
  ci <- c((m2-m1)+error, (m2-m1)-error)
  print(list("z statistic" = z, "confidence interval" = ci, "p value" = p))
  }


  }

Z.prop.test(x, p0, y)
```

#Problem 2
###Fit the regression model and, using {ggplot2}, produce a scatterplot with the fitted line superimposed upon the data. Append the the fitted model equation to your plot
```{r}
library(tidyverse)
library(curl)
f <- curl("https://raw.githubusercontent.com/difiore/ADA-2019/master/KamilarAndCooperData.csv")
d <- read.csv(f, header = TRUE, stringsAsFactors = FALSE)
d <- na.omit(d)

longevity <-(d$MaxLongevity_m)
brain_size<-(d$Brain_Size_Species_Mean)

regmod <- lm(MaxLongevity_m ~ Brain_Size_Species_Mean, data = d)
regmod
head(regmod$model)

library(ggplot2)
g <- ggplot(data = d, aes(x = brain_size, y = longevity))
g <- g + geom_point()
g <- g + geom_smooth(method = "lm", formula = y ~ x)
g

#log
loglong<- log(longevity)
logbrain<- log(brain_size)
logregmod<-lm(loglong ~ logbrain)
logregmod
head(logregmod$model)

library(ggplot2)
g2 <- ggplot(data = d, aes(x = log(brain_size), y = log(longevity)))
g2 <- g2 + geom_point()
g2 <- g2 + geom_smooth(method = "lm", formula = y ~ x)
g2
```

##Beta1
```{r}
beta1 <- cov(brain_size, longevity)/var(brain_size)
beta1
#There is a positive correlation between the variables such that beta1 does not equal 0.

#log
beta2 <- cov(logbrain, loglong)/var(logbrain)
beta2
#There is a positive correlation between the variables such that beta1 does not equal 0.
```
##CI
```{r}
summary(regmod)
t <- coef(summary(regmod))
t <- data.frame(unlist(t))
colnames(t) <- c("Est", "SE", "t", "p")
t 

alpha <- 0.1
t$redmodCI <- confint(regmod, level = 1 - alpha)
lowerCI <- t$Est - qt(1 - alpha/2, df = 24) * t$SE
upperCI <- t$Est + qt(1 - alpha/2, df = 24) * t$SE
t$CI <- cbind(lowerCI, upperCI)
dimnames(t$CI)[[1]] <- c("(Intercept)", "brain_size")
dimnames(t$CI)[[2]] <- c("(5 %)", "(95 %)")
t[ ,5:6]

#log
summary(logregmod)
s <- coef(summary(logregmod))
s <- data.frame(unlist(s))
colnames(s) <- c("Est", "SE", "t", "p")
s 

s$logregmodCI <- confint(logregmod, level = 1 - alpha)
lowerCI <- s$Est - qt(1 - alpha/2, df = 24) * s$SE
upperCI <- s$Est + qt(1 - alpha/2, df = 24) * s$SE
s$CI <- cbind(lowerCI, upperCI)
dimnames(s$CI)[[1]] <- c("(Intercept)", "brain_size")
dimnames(s$CI)[[2]] <- c("(5 %)", "(95 %)")
s[ ,5:6]
```

###Using your model, add lines for the 90% confidence and prediction interval bands on the plot, and add a legend to differentiate between the lines.
```{r}
h_hat <- predict(regmod, newdata = data.frame(Brain_Size_Species_Mean = d$Brain_Size_Species_Mean))
df <- data.frame(cbind(d$Brain_Size_Species_Mean, d$MaxLongevity_m, h_hat))
ci <- predict(regmod, newdata = data.frame(Brain_Size_Species_Mean = d$Brain_Size_Species_Mean), interval = "confidence", 
              level = 0.90)
df <- cbind(df, ci)
names(df) <- c("brain_size", "longevity", "yhat", "CIfit", "CIlwr", "CIupr")
g <- ggplot(data = df, aes(x = brain_size, y = longevity))
g <- g + geom_point(alpha = 0.5)
g <- g + geom_line(aes(x = brain_size, y = CIfit), colour = "black")
g <- g + geom_line(aes(x = brain_size, y = CIlwr), colour = "blue")
g <- g + geom_line(aes(x = brain_size, y = CIupr), colour = "blue")
g 
pi <- predict(regmod, newdata = data.frame(Brain_Size_Species_Mean = d$Brain_Size_Species_Mean), interval = "prediction", 
              level = 0.90)
df <- cbind(df, pi)
g <- g + geom_line(data = df, aes(x = brain_size, y = lwr), colour = "red")
g <- g + geom_line(data = df, aes(x = brain_size, y = upr), colour = "red")
g + geom_text("Blue= CI, Red = Prediction interval")
g +
  annotate("text", x=100, y=800, label= "Blue=CI, Red=Prediction interval") 
#log

g_hat <- predict(logregmod, newdata = data.frame(logbrain = logbrain))
df2 <- data.frame(cbind(logbrain, loglong, g_hat))
ci2 <- predict(logregmod, newdata = data.frame(logbrain = logbrain), interval = "confidence", 
               level = 0.90)
df2 <- cbind(df2, ci2)
names(df2) <- c("logbrain", "loglong", "ghat", "CIfit", "CIlwr", "CIupr")
g2 <- ggplot(data = df2, aes(x = logbrain, y = loglong))
g2 <- g2 + geom_point(alpha = 0.5)
g2 <- g2 + geom_line(aes(x = logbrain, y = CIfit), colour = "black")
g2 <- g2 + geom_line(aes(x = logbrain, y = CIlwr), colour = "blue")
g2 <- g2 + geom_line(aes(x = logbrain, y = CIupr), colour = "blue")
g2
pi2 <- predict(logregmod, newdata = data.frame(logbrain = logbrain), interval = "prediction", 
               level = 0.90)
df2 <- cbind(df2, pi2)
g2 <- g2 + geom_line(data = df2, aes(x = logbrain, y = lwr), colour = "red")
g2 <- g2 + geom_line(data = df2, aes(x = logbrain, y = upr), colour = "red")
g2 +
  annotate("text", x=3, y=6.5, label= "Blue=CI, Red=Prediction interval") 

```


```{r}
pi <- predict(regmod, newdata = data.frame(Brain_Size_Species_Mean = 800), interval = "prediction", 
              level = 0.90)
pi
#I dont' trust this prediction because there are not alot of data points nearby.

#log

pi2 <- predict(logregmod, newdata = data.frame(logbrain = log(800)), interval = "prediction", 
              level = 0.90)
pi2
#I would trust this prediction more than the one above because there are more data points closer.
```

I think the linea model of the log of the data is better because the dta points are more evenly spread out making the model more accurate. 
